model:
  max_seq_len: 1024  # Maximum sequence length for position embeddings
  vocab_size: 50257  # Vocabulary size for token embedding
  embed_dim: 768     # Embedding dimension for tokens
  num_layers: 12     # Number of transformer blocks
  num_heads: 12      # Number of attention heads

training:
  batch_size: 32     # Batch size for training
  learning_rate: 3e-5  # Learning rate
  max_steps: 1000000  # Maximum training steps
  warmup_steps: 10000  # Warmup steps for learning rate scheduling
  optimizer: "AdamW"  # Optimizer type
  beta1: 0.9         # Beta1 for Adam optimizer
  beta2: 0.999       # Beta2 for Adam optimizer
  epsilon: 1e-8      # Epsilon for optimizer
  weight_decay: 0.1  # Weight decay for optimizer
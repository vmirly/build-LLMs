model:
  combined_proj: true  # Whether to combine token and position embeddings
  flash_attention: true  # Whether to use Flash attention
  max_seq_len: 1024  # Maximum sequence length for position embeddings
  vocab_size: 50257  # Vocabulary size for token embedding
  embed_dim: 768     # Embedding dimension for tokens
  num_layers: 12     # Number of transformer blocks
  num_heads: 12      # Number of attention heads
  p_drop_hidden: 0.1 # Dropout probability for hidden states
  p_drop_attn: 0.1   # Dropout probability for attention weights
  p_drop_embed: 0.1  # Dropout probability for token embeddings
  p_drop_resid: 0.1  # Dropout probability for residual connections

training:
  dataset:
    train_dir: "data"
    val_dir: "data"
  batch_size: 16       # Batch size for training
  total_batch_tokens: 524288
  max_lr: 6e-4         # Learning rate
  min_lr: 6e-5         # Minimum learning rate
  max_steps: 19073     # Maximum training steps
  warmup_steps: 100    # Warmup steps for learning rate scheduling
  optimizer: "AdamW"   # Optimizer type
  beta1: 0.9           # Beta1 for Adam optimizer
  beta2: 0.95          # Beta2 for Adam optimizer
  epsilon: 1e-8        # Epsilon for optimizer
  weight_decay: 0.1    # Weight decay for optimizer

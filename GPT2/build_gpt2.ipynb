{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.4.1\n",
      "transformers: 4.45.2\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"torch\", \"transformers\"\n",
    "]\n",
    "\n",
    "for pkg in pkgs:\n",
    "    print(f\"{pkg}: {version(pkg)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import yaml\n",
    "\n",
    "from pydantic import BaseModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelConfig(hf_model_name='', vocab_size=50257, max_seq_len=1024, embed_dim=768, num_heads=12, num_layers=12, attn_dropout=0.1, resid_dropout=0.1, embed_dropout=0.1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ModelConfig(BaseModel):\n",
    "    hf_model_name: str = \"\"\n",
    "    vocab_size: int = 50257\n",
    "    max_seq_len: int = 1024\n",
    "    embed_dim: int = 768\n",
    "    num_heads: int = 12\n",
    "    num_layers: int = 12\n",
    "    attn_dropout: float = 0.1\n",
    "    resid_dropout: float = 0.1\n",
    "    embed_dropout: float = 0.1\n",
    "\n",
    "# read in the config.yaml file\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "model_config = ModelConfig(**config)\n",
    "model_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multihead (Causal) Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CausalSelfAttention(\n",
      "  (q_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (k_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (v_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Output: torch.Size([2, 5, 32])\n"
     ]
    }
   ],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Causal self-attention layer, masking the future tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.num_heads = cfg.num_heads\n",
    "        self.q_proj = nn.Linear(cfg.embed_dim, cfg.embed_dim)\n",
    "        self.k_proj = nn.Linear(cfg.embed_dim, cfg.embed_dim)\n",
    "        self.v_proj = nn.Linear(cfg.embed_dim, cfg.embed_dim)\n",
    "        self.out_proj = nn.Linear(cfg.embed_dim, cfg.embed_dim)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(0.1)\n",
    "        self.resid_dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # Create a bias tensor to prevent attention to future tokens\n",
    "        mask = torch.tril(torch.ones(cfg.max_seq_len, cfg.max_seq_len))\n",
    "        self.register_buffer(\n",
    "            'mask', (mask == 0).view(1, 1, cfg.max_seq_len, cfg.max_seq_len)\n",
    "        )\n",
    "        # mask will be a tensor like the following:\n",
    "        # tensor([[[[False, True,  True,  ...,  True],\n",
    "        #           [False, False, True,  ...,  True],\n",
    "        #           [False, False, False, ...,  True],\n",
    "        #           ...,\n",
    "        #           [False, False, False, ..., False]]]])\n",
    "        # where True values indicate that the token should be masked\n",
    "        # i.e., replaced with -inf in the attention scores\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply linear transformations to get queries, keys, and values\n",
    "        # x: [B, T, C]\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        # q,k,v: [B, T, C]\n",
    "\n",
    "        # Split the queries, keys, and values into multiple heads\n",
    "        B, T, C = q.size()\n",
    "        q = q.view(B, T, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        k = k.view(B, T, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        v = v.view(B, T, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        # q,k,v: [B, nh, T, C//nh]\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        scores = torch.matmul(q, k.permute(0, 1, 3, 2))\n",
    "        scores = scores / (math.sqrt(k.size(-1)))\n",
    "        scores.masked_fill_(self.mask[:, :, :T, :T], -torch.inf)\n",
    "        # scores: [B, nh, T, T]\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        # attn_weights: [B, nh, T, T]\n",
    "\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "        \n",
    "        # Multiply attention weights with values\n",
    "        out = torch.matmul(attn_weights, v)\n",
    "        # out: [B, nh, T, C//nh]\n",
    "\n",
    "        # Concatenate the heads and apply a linear transformation\n",
    "        out = out.permute(0, 2, 1, 3).contiguous().view(B, T, C)\n",
    "        out = self.out_proj(out)\n",
    "        # out: [B, T, C]\n",
    "\n",
    "        out = self.resid_dropout(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# testing\n",
    "cfg = ModelConfig(\n",
    "    vocab_size=100,\n",
    "    max_seq_len=10,\n",
    "    embed_dim=32,\n",
    "    num_heads=8,\n",
    "    num_layers=2,\n",
    "    attn_dropout=0.1,\n",
    "    resid_dropout=0.1,\n",
    "    hidden_dropout=0.1\n",
    ")\n",
    "x = torch.randn(2, 5, cfg.embed_dim)\n",
    "mha = CausalSelfAttention(cfg)\n",
    "print(mha)\n",
    "\n",
    "out = mha(x)\n",
    "print(\"\\nOutput:\", out.size())  # torch.Size([2, 5, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward Network (FFN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedForwardNetwork(\n",
      "  (fc1): Linear(in_features=32, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (gelu): GELU(approximate='tanh')\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Output: torch.Size([2, 5, 32])\n"
     ]
    }
   ],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        embed_dim = cfg.embed_dim\n",
    "        hidden_dim = cfg.embed_dim * 4\n",
    "        p_drop = cfg.resid_dropout\n",
    "        # Two linear layers with activation in between\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.resid_dropout = nn.Dropout(p_drop)\n",
    "\n",
    "    def forward(self, x):            # [B, T, C]\n",
    "        x = self.gelu(self.fc1(x))  # [B, T, 2C]\n",
    "        x = self.fc2(x)              # [B, T, C]\n",
    "        x = self.resid_dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# testing\n",
    "ffn = FeedForwardNetwork(cfg)\n",
    "print(ffn)\n",
    "x = torch.randn(2, 5, cfg.embed_dim)\n",
    "out = ffn(x)\n",
    "print(\"\\nOutput:\", out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerBlock(\n",
      "  (mha): CausalSelfAttention(\n",
      "    (q_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (k_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (v_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  (ffn): FeedForwardNetwork(\n",
      "    (fc1): Linear(in_features=32, out_features=128, bias=True)\n",
      "    (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "    (gelu): GELU(approximate='tanh')\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Input: torch.Size([2, 5, 32])\n",
      "Output: torch.Size([2, 5, 32])\n"
     ]
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.mha = CausalSelfAttention(config)\n",
    "        self.ln1 = nn.LayerNorm(config.embed_dim)\n",
    "        self.ffn = FeedForwardNetwork(config)\n",
    "        self.ln2 = nn.LayerNorm(config.embed_dim)\n",
    "\n",
    "        self.resid_dropout = nn.Dropout(config.resid_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply self-attention and add residual connection\n",
    "        shortcut = x\n",
    "        x = self.ln1(x)\n",
    "        x = self.mha(x)[0]\n",
    "        x = self.resid_dropout(x)\n",
    "        x = shortcut + x\n",
    "\n",
    "        # Apply feedforward network and add residual connection\n",
    "        shortcut = x\n",
    "        x = self.ln2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = self.resid_dropout(x)\n",
    "        x = shortcut + x\n",
    "\n",
    "        return x\n",
    "    \n",
    "# testing\n",
    "transformer_block = TransformerBlock(cfg)\n",
    "print(transformer_block)\n",
    "x = torch.randn(2, 5, cfg.embed_dim)\n",
    "out = transformer_block(x)\n",
    "print(\"\\nInput:\", x.size())\n",
    "print(\"Output:\", out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2(\n",
      "  (token_emb): Embedding(100, 32)\n",
      "  (pos_emb): Embedding(10, 32)\n",
      "  (embed_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (layers): ModuleList(\n",
      "    (0-1): 2 x TransformerBlock(\n",
      "      (mha): CausalSelfAttention(\n",
      "        (q_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (k_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (v_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): FeedForwardNetwork(\n",
      "        (fc1): Linear(in_features=32, out_features=128, bias=True)\n",
      "        (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "        (gelu): GELU(approximate='tanh')\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (ln_final): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  (head): Linear(in_features=32, out_features=100, bias=False)\n",
      ")\n",
      "\n",
      "Input: torch.Size([2, 5])\n",
      "Output: torch.Size([2, 5, 100])\n"
     ]
    }
   ],
   "source": [
    "class GPT2(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        embed_dim = config.embed_dim\n",
    "        vocab_size = config.vocab_size\n",
    "        context_length = config.max_seq_len\n",
    "        self.num_layers = config.num_layers\n",
    "\n",
    "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_emb = nn.Embedding(context_length, embed_dim)\n",
    "        self.embed_dropout = nn.Dropout(config.embed_dropout)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(config) for _ in range(self.num_layers)\n",
    "        ])\n",
    "\n",
    "        self.ln_final = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "\n",
    "        # weight sharing -> saving 40M parameters\n",
    "        self.head.weight = self.token_emb.weight\n",
    "\n",
    "    def forward(self, idx):\n",
    "        # Generate token embeddings\n",
    "        tok_emb = self.token_emb(idx)\n",
    "        # Generate position embeddings\n",
    "        pos = torch.arange(idx.size(1), device=idx.device).unsqueeze(0)\n",
    "        pos_emb = self.pos_emb(pos)\n",
    "        x = self.embed_dropout(tok_emb + pos_emb)\n",
    "\n",
    "        # Apply the transformer blocks\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Apply the final layer norm\n",
    "        x = self.ln_final(x)\n",
    "\n",
    "        # Generate logits\n",
    "        logits = self.head(x)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "# testing\n",
    "gpt2 = GPT2(cfg)\n",
    "print(gpt2)\n",
    "x = torch.randint(0, 100, (2, 5))\n",
    "out = gpt2(x)\n",
    "print(\"\\nInput:\", x.size())\n",
    "print(\"Output:\", out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading HuggingFace ðŸ¤— weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Model(\n",
      "  (wte): Embedding(50257, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0-11): 12 x GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Model\n",
    "\n",
    "gpt2_hf = GPT2Model.from_pretrained(\"gpt2\")\n",
    "print(gpt2_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wte.weight',\n",
      " 'wpe.weight',\n",
      " 'h.0.ln_1.weight',\n",
      " 'h.0.ln_1.bias',\n",
      " 'h.0.attn.c_attn.weight',\n",
      " 'h.0.attn.c_attn.bias',\n",
      " 'h.0.attn.c_proj.weight',\n",
      " 'h.0.attn.c_proj.bias',\n",
      " 'h.0.ln_2.weight',\n",
      " 'h.0.ln_2.bias',\n",
      " 'h.0.mlp.c_fc.weight',\n",
      " 'h.0.mlp.c_fc.bias',\n",
      " 'h.0.mlp.c_proj.weight',\n",
      " 'h.0.mlp.c_proj.bias',\n",
      " 'h.1.ln_1.weight',\n",
      " 'h.1.ln_1.bias',\n",
      " 'h.1.attn.c_attn.weight',\n",
      " 'h.1.attn.c_attn.bias',\n",
      " 'h.1.attn.c_proj.weight',\n",
      " 'h.1.attn.c_proj.bias',\n",
      " 'h.1.ln_2.weight',\n",
      " 'h.1.ln_2.bias',\n",
      " 'h.1.mlp.c_fc.weight',\n",
      " 'h.1.mlp.c_fc.bias',\n",
      " 'h.1.mlp.c_proj.weight',\n",
      " 'h.1.mlp.c_proj.bias',\n",
      " 'h.2.ln_1.weight',\n",
      " 'h.2.ln_1.bias',\n",
      " 'h.2.attn.c_attn.weight',\n",
      " 'h.2.attn.c_attn.bias',\n",
      " 'h.2.attn.c_proj.weight',\n",
      " 'h.2.attn.c_proj.bias',\n",
      " 'h.2.ln_2.weight',\n",
      " 'h.2.ln_2.bias',\n",
      " 'h.2.mlp.c_fc.weight',\n",
      " 'h.2.mlp.c_fc.bias',\n",
      " 'h.2.mlp.c_proj.weight',\n",
      " 'h.2.mlp.c_proj.bias',\n",
      " 'h.3.ln_1.weight',\n",
      " 'h.3.ln_1.bias',\n",
      " 'h.3.attn.c_attn.weight',\n",
      " 'h.3.attn.c_attn.bias',\n",
      " 'h.3.attn.c_proj.weight',\n",
      " 'h.3.attn.c_proj.bias',\n",
      " 'h.3.ln_2.weight',\n",
      " 'h.3.ln_2.bias',\n",
      " 'h.3.mlp.c_fc.weight',\n",
      " 'h.3.mlp.c_fc.bias',\n",
      " 'h.3.mlp.c_proj.weight',\n",
      " 'h.3.mlp.c_proj.bias',\n",
      " 'h.4.ln_1.weight',\n",
      " 'h.4.ln_1.bias',\n",
      " 'h.4.attn.c_attn.weight',\n",
      " 'h.4.attn.c_attn.bias',\n",
      " 'h.4.attn.c_proj.weight',\n",
      " 'h.4.attn.c_proj.bias',\n",
      " 'h.4.ln_2.weight',\n",
      " 'h.4.ln_2.bias',\n",
      " 'h.4.mlp.c_fc.weight',\n",
      " 'h.4.mlp.c_fc.bias',\n",
      " 'h.4.mlp.c_proj.weight',\n",
      " 'h.4.mlp.c_proj.bias',\n",
      " 'h.5.ln_1.weight',\n",
      " 'h.5.ln_1.bias',\n",
      " 'h.5.attn.c_attn.weight',\n",
      " 'h.5.attn.c_attn.bias',\n",
      " 'h.5.attn.c_proj.weight',\n",
      " 'h.5.attn.c_proj.bias',\n",
      " 'h.5.ln_2.weight',\n",
      " 'h.5.ln_2.bias',\n",
      " 'h.5.mlp.c_fc.weight',\n",
      " 'h.5.mlp.c_fc.bias',\n",
      " 'h.5.mlp.c_proj.weight',\n",
      " 'h.5.mlp.c_proj.bias',\n",
      " 'h.6.ln_1.weight',\n",
      " 'h.6.ln_1.bias',\n",
      " 'h.6.attn.c_attn.weight',\n",
      " 'h.6.attn.c_attn.bias',\n",
      " 'h.6.attn.c_proj.weight',\n",
      " 'h.6.attn.c_proj.bias',\n",
      " 'h.6.ln_2.weight',\n",
      " 'h.6.ln_2.bias',\n",
      " 'h.6.mlp.c_fc.weight',\n",
      " 'h.6.mlp.c_fc.bias',\n",
      " 'h.6.mlp.c_proj.weight',\n",
      " 'h.6.mlp.c_proj.bias',\n",
      " 'h.7.ln_1.weight',\n",
      " 'h.7.ln_1.bias',\n",
      " 'h.7.attn.c_attn.weight',\n",
      " 'h.7.attn.c_attn.bias',\n",
      " 'h.7.attn.c_proj.weight',\n",
      " 'h.7.attn.c_proj.bias',\n",
      " 'h.7.ln_2.weight',\n",
      " 'h.7.ln_2.bias',\n",
      " 'h.7.mlp.c_fc.weight',\n",
      " 'h.7.mlp.c_fc.bias',\n",
      " 'h.7.mlp.c_proj.weight',\n",
      " 'h.7.mlp.c_proj.bias',\n",
      " 'h.8.ln_1.weight',\n",
      " 'h.8.ln_1.bias',\n",
      " 'h.8.attn.c_attn.weight',\n",
      " 'h.8.attn.c_attn.bias',\n",
      " 'h.8.attn.c_proj.weight',\n",
      " 'h.8.attn.c_proj.bias',\n",
      " 'h.8.ln_2.weight',\n",
      " 'h.8.ln_2.bias',\n",
      " 'h.8.mlp.c_fc.weight',\n",
      " 'h.8.mlp.c_fc.bias',\n",
      " 'h.8.mlp.c_proj.weight',\n",
      " 'h.8.mlp.c_proj.bias',\n",
      " 'h.9.ln_1.weight',\n",
      " 'h.9.ln_1.bias',\n",
      " 'h.9.attn.c_attn.weight',\n",
      " 'h.9.attn.c_attn.bias',\n",
      " 'h.9.attn.c_proj.weight',\n",
      " 'h.9.attn.c_proj.bias',\n",
      " 'h.9.ln_2.weight',\n",
      " 'h.9.ln_2.bias',\n",
      " 'h.9.mlp.c_fc.weight',\n",
      " 'h.9.mlp.c_fc.bias',\n",
      " 'h.9.mlp.c_proj.weight',\n",
      " 'h.9.mlp.c_proj.bias',\n",
      " 'h.10.ln_1.weight',\n",
      " 'h.10.ln_1.bias',\n",
      " 'h.10.attn.c_attn.weight',\n",
      " 'h.10.attn.c_attn.bias',\n",
      " 'h.10.attn.c_proj.weight',\n",
      " 'h.10.attn.c_proj.bias',\n",
      " 'h.10.ln_2.weight',\n",
      " 'h.10.ln_2.bias',\n",
      " 'h.10.mlp.c_fc.weight',\n",
      " 'h.10.mlp.c_fc.bias',\n",
      " 'h.10.mlp.c_proj.weight',\n",
      " 'h.10.mlp.c_proj.bias',\n",
      " 'h.11.ln_1.weight',\n",
      " 'h.11.ln_1.bias',\n",
      " 'h.11.attn.c_attn.weight',\n",
      " 'h.11.attn.c_attn.bias',\n",
      " 'h.11.attn.c_proj.weight',\n",
      " 'h.11.attn.c_proj.bias',\n",
      " 'h.11.ln_2.weight',\n",
      " 'h.11.ln_2.bias',\n",
      " 'h.11.mlp.c_fc.weight',\n",
      " 'h.11.mlp.c_fc.bias',\n",
      " 'h.11.mlp.c_proj.weight',\n",
      " 'h.11.mlp.c_proj.bias',\n",
      " 'ln_f.weight',\n",
      " 'ln_f.bias']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(list(gpt2_hf.state_dict().keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hf_weights(model, hf_model):\n",
    "    # Load the weights from the Hugging Face model\n",
    "    hf_dict = hf_model.state_dict()\n",
    "\n",
    "    # assign emnbedding weights\n",
    "    model.token_emb.weight.data.copy_(hf_dict['wte.weight'])\n",
    "    model.pos_emb.weight.data.copy_(hf_dict['wpe.weight'])\n",
    "\n",
    "    # assign transformer block weights\n",
    "    for idx, layer in enumerate(model.layers):\n",
    "        # MHA weights and biases\n",
    "        qkv_weight = hf_dict[f'h.{idx}.attn.c_attn.weight']\n",
    "        qkv_bias = hf_dict[f'h.{idx}.attn.c_attn.bias']\n",
    "        q_weight, k_weight, v_weight = qkv_weight.chunk(3, dim=1)\n",
    "        q_bias, k_bias, v_bias = qkv_bias.chunk(3)\n",
    "        layer.mha.q_proj.weight.data.copy_(q_weight.T)\n",
    "        layer.mha.q_proj.bias.data.copy_(q_bias)\n",
    "        layer.mha.k_proj.weight.data.copy_(k_weight.T)\n",
    "        layer.mha.k_proj.bias.data.copy_(k_bias)\n",
    "        layer.mha.v_proj.weight.data.copy_(v_weight.T)\n",
    "        layer.mha.v_proj.bias.data.copy_(v_bias)\n",
    "        # MHA out projection weights and biases\n",
    "        c_weight = hf_dict[f'h.{idx}.attn.c_proj.weight']\n",
    "        c_bias = hf_dict[f'h.{idx}.attn.c_proj.bias']\n",
    "        layer.mha.out_proj.weight.data.copy_(c_weight.T)\n",
    "        layer.mha.out_proj.bias.data.copy_(c_bias)\n",
    "        # Layer norm weights and biases\n",
    "        layer.ln1.weight.data.copy_(hf_dict[f'h.{idx}.ln_1.weight'])\n",
    "        layer.ln1.bias.data.copy_(hf_dict[f'h.{idx}.ln_1.bias'])\n",
    "        layer.ln2.weight.data.copy_(hf_dict[f'h.{idx}.ln_2.weight'])\n",
    "        layer.ln2.bias.data.copy_(hf_dict[f'h.{idx}.ln_2.bias'])\n",
    "        # FFN weights and biases\n",
    "        layer.ffn.fc1.weight.data.copy_(hf_dict[f'h.{idx}.mlp.c_fc.weight'].T)\n",
    "        layer.ffn.fc1.bias.data.copy_(hf_dict[f'h.{idx}.mlp.c_fc.bias'])\n",
    "        layer.ffn.fc2.weight.data.copy_(hf_dict[f'h.{idx}.mlp.c_proj.weight'].T)\n",
    "        layer.ffn.fc2.bias.data.copy_(hf_dict[f'h.{idx}.mlp.c_proj.bias'])\n",
    "\n",
    "    # assign final layer norm weights\n",
    "    model.ln_final.weight.data.copy_(hf_dict['ln_f.weight'])\n",
    "    model.ln_final.bias.data.copy_(hf_dict['ln_f.bias'])\n",
    "    # assign head weights (wte)\n",
    "    #model.head.weight.data.copy_(hf_dict['wte.weight'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 163037184\n"
     ]
    }
   ],
   "source": [
    "gpt2 = GPT2(model_config)\n",
    "\n",
    "# count number of parameters\n",
    "num_params = sum(p.numel() for p in gpt2.parameters())\n",
    "print(f\"Number of parameters: {num_params}\")\n",
    "\n",
    "load_hf_weights(gpt2, gpt2_hf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I'm a language model, not rendering an object. Though later in the installation process my code will probably try to render the map\n"
     ]
    }
   ],
   "source": [
    "def generate_text(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompt,\n",
    "        max_len=100,\n",
    "        temperature=1.0,\n",
    "    ):\n",
    "    model.eval()\n",
    "    prompt = tokenizer.encode(prompt)\n",
    "    prompt = torch.tensor(prompt).unsqueeze(0)\n",
    "    generated = prompt\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            logits = model(generated)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "    return tokenizer.decode(generated[0].tolist())\n",
    "\n",
    "# testing\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "torch.manual_seed(123)\n",
    "\n",
    "prompt = \"Hello, I'm a language model,\"\n",
    "text = generate_text(gpt2, tokenizer, prompt, max_len=20)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gpt2.state_dict(), \"gpt2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.4.1\n",
      "transformers: 4.45.2\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"torch\", \"transformers\"\n",
    "]\n",
    "\n",
    "for pkg in pkgs:\n",
    "    print(f\"{pkg}: {version(pkg)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import yaml\n",
    "\n",
    "from pydantic import BaseModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelConfig(max_seq_len=1024, embed_dim=768, num_heads=12, num_layers=12, attn_dropout=0.1, resid_dropout=0.1, hidden_dropout=0.1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ModelConfig(BaseModel):\n",
    "    max_seq_len: int = 1024\n",
    "    embed_dim: int = 768\n",
    "    num_heads: int = 12\n",
    "    num_layers: int = 12\n",
    "    attn_dropout: float = 0.1\n",
    "    resid_dropout: float = 0.1\n",
    "    hidden_dropout: float = 0.1\n",
    "\n",
    "# read in the config.yaml file\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "model_config = ModelConfig(**config)\n",
    "model_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multihead (Causal) Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CausalSelfAttention(\n",
      "  (q_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (k_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (v_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Output: torch.Size([2, 5, 32])\n"
     ]
    }
   ],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Causal self-attention layer, masking the future tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.num_heads = cfg.num_heads\n",
    "        self.q_proj = nn.Linear(cfg.embed_dim, cfg.embed_dim)\n",
    "        self.k_proj = nn.Linear(cfg.embed_dim, cfg.embed_dim)\n",
    "        self.v_proj = nn.Linear(cfg.embed_dim, cfg.embed_dim)\n",
    "        self.out_proj = nn.Linear(cfg.embed_dim, cfg.embed_dim)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(0.1)\n",
    "        self.resid_dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # Create a bias tensor to prevent attention to future tokens\n",
    "        mask = torch.tril(torch.ones(cfg.max_seq_len, cfg.max_seq_len))\n",
    "        self.register_buffer(\n",
    "            'mask', (mask == 0).view(1, 1, cfg.max_seq_len, cfg.max_seq_len)\n",
    "        )\n",
    "        # mask will be a tensor like the following:\n",
    "        # tensor([[[[False, True,  True,  ...,  True],\n",
    "        #           [False, False, True,  ...,  True],\n",
    "        #           [False, False, False, ...,  True],\n",
    "        #           ...,\n",
    "        #           [False, False, False, ..., False]]]])\n",
    "        # where True values indicate that the token should be masked\n",
    "        # i.e., replaced with -inf in the attention scores\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply linear transformations to get queries, keys, and values\n",
    "        # x: [B, T, C]\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        # q,k,v: [B, T, C]\n",
    "\n",
    "        # Split the queries, keys, and values into multiple heads\n",
    "        B, T, C = q.size()\n",
    "        q = q.view(B, T, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        k = k.view(B, T, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        v = v.view(B, T, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        # q,k,v: [B, nh, T, C//nh]\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        scores = torch.matmul(q, k.permute(0, 1, 3, 2))\n",
    "        scores = scores / (math.sqrt(k.size(-1)))\n",
    "        scores.masked_fill_(self.mask[:, :, :T, :T], -torch.inf)\n",
    "        # scores: [B, nh, T, T]\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        # attn_weights: [B, nh, T, T]\n",
    "\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "        \n",
    "        # Multiply attention weights with values\n",
    "        out = torch.matmul(attn_weights, v)\n",
    "        # out: [B, nh, T, C//nh]\n",
    "\n",
    "        # Concatenate the heads and apply a linear transformation\n",
    "        out = out.permute(0, 2, 1, 3).contiguous().view(B, T, C)\n",
    "        out = self.out_proj(out)\n",
    "        # out: [B, T, C]\n",
    "\n",
    "        out = self.resid_dropout(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# testing\n",
    "cfg = ModelConfig(\n",
    "    max_seq_len=10,\n",
    "    embed_dim=32,\n",
    "    num_heads=8,\n",
    "    num_layers=2,\n",
    "    attn_dropout=0.1,\n",
    "    resid_dropout=0.1,\n",
    "    hidden_dropout=0.1\n",
    ")\n",
    "x = torch.randn(2, 5, cfg.embed_dim)\n",
    "mha = CausalSelfAttention(cfg)\n",
    "print(mha)\n",
    "\n",
    "out = mha(x)\n",
    "print(\"\\nOutput:\", out.size())  # torch.Size([2, 5, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward Network (FFN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedForwardNetwork(\n",
      "  (fc1): Linear(in_features=32, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (gelu): GELU(approximate='tanh')\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Output: torch.Size([2, 5, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        embed_dim = cfg.embed_dim\n",
    "        hidden_dim = cfg.embed_dim * 4\n",
    "        p_drop = cfg.hidden_dropout\n",
    "        # Two linear layers with activation in between\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.resid_dropout = nn.Dropout(p_drop)\n",
    "\n",
    "    def forward(self, x):            # [B, T, C]\n",
    "        x = self.gelu(self.fc1(x))  # [B, T, 2C]\n",
    "        x = self.fc2(x)              # [B, T, C]\n",
    "        x = self.resid_dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# testing\n",
    "ffn = FeedForwardNetwork(cfg)\n",
    "print(ffn)\n",
    "x = torch.randn(2, 5, cfg.embed_dim)\n",
    "out = ffn(x)\n",
    "print(\"\\nOutput:\", out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerBlock(\n",
      "  (mha): CausalSelfAttention(\n",
      "    (q_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (k_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (v_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  (ffn): FeedForwardNetwork(\n",
      "    (fc1): Linear(in_features=32, out_features=128, bias=True)\n",
      "    (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "    (gelu): GELU(approximate='tanh')\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Input: torch.Size([2, 5, 32])\n",
      "Output: torch.Size([2, 5, 32])\n"
     ]
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.mha = CausalSelfAttention(config)\n",
    "        self.ln1 = nn.LayerNorm(config.embed_dim)\n",
    "        self.ffn = FeedForwardNetwork(config)\n",
    "        self.ln2 = nn.LayerNorm(config.embed_dim)\n",
    "\n",
    "        self.resid_dropout = nn.Dropout(config.resid_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply self-attention and add residual connection\n",
    "        shortcut = x\n",
    "        x = self.ln1(x)\n",
    "        x = self.mha(x)[0]\n",
    "        x = self.resid_dropout(x)\n",
    "        x = shortcut + x\n",
    "\n",
    "        # Apply feedforward network and add residual connection\n",
    "        shortcut = x\n",
    "        x = self.ln2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = self.resid_dropout(x)\n",
    "        x = shortcut + x\n",
    "\n",
    "        return x\n",
    "    \n",
    "# testing\n",
    "transformer_block = TransformerBlock(cfg)\n",
    "print(transformer_block)\n",
    "x = torch.randn(2, 5, cfg.embed_dim)\n",
    "out = transformer_block(x)\n",
    "print(\"\\nInput:\", x.size())\n",
    "print(\"Output:\", out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
